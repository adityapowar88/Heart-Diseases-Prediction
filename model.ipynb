{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88fa7103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffa64439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c14cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63379f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "# test = pd.read_csv(\"D:/Python/linkdinln/HateSpeechDetection/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11625200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c593d1a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241m.\u001b[39mtail()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3803fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(train[\"label\"] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc74d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(train[\"label\"] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d05b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51b381c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "870c0e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45d9f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])|(\\%)|(\\$)|(\\>)|(\\<)|(\\{)|(\\})\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s/><br\\s/?)|(-)|(/)|(:).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b4d4db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7f58c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(df):\n",
    "  tempArr = []\n",
    "  for line in df:\n",
    "    # send to tweet_processor\n",
    "    tmpL = p.clean(line)\n",
    "    # remove puctuation\n",
    "    tmpL = REPLACE_NO_SPACE.sub(\"\", tmpL.lower()) # convert all tweets to lower cases\n",
    "    tmpL = REPLACE_WITH_SPACE.sub(\" \", tmpL)\n",
    "    tempArr.append(tmpL)\n",
    "  return tempArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "68db1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet = clean_tweets(train[\"tweet\"])\n",
    "train_tweet = pd.DataFrame(train_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6283f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0a5e14fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_tweets(df):\n",
    "#     tempArr = []\n",
    "#     tmpL = p.clean(df)\n",
    "#     # remove puctuation\n",
    "#     tmpL = REPLACE_NO_SPACE.sub(\"\", tmpL.lower()) # convert all tweets to lower cases\n",
    "#     tmpL = REPLACE_WITH_SPACE.sub(\" \", tmpL)\n",
    "#     tempArr.append(tmpL)\n",
    "#     return tempArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ff068477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweets('#this is@ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d960c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f44c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"clean_tweet\"] = train_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "53b09658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #cnn calls #michigan middle school 'buil...</td>\n",
       "      <td>calls middle school build the wall chant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>no comment!  in #australia   #opkillingbay #se...</td>\n",
       "      <td>no comment in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>retweet if you agree!</td>\n",
       "      <td>retweet if you agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>@user @user lumpy says i am a . prove it lumpy.</td>\n",
       "      <td>lumpy says i am a  prove it lumpy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>it's unbelievable that in the 21st century we'...</td>\n",
       "      <td>its unbelievable that in the st century wed ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31934</th>\n",
       "      <td>31935</td>\n",
       "      <td>1</td>\n",
       "      <td>lady banned from kentucky mall. @user  #jcpenn...</td>\n",
       "      <td>lady banned from kentucky mall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31946</th>\n",
       "      <td>31947</td>\n",
       "      <td>1</td>\n",
       "      <td>@user omfg i'm offended! i'm a  mailbox and i'...</td>\n",
       "      <td>omfg im offended im a mailbox and im proud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31947</th>\n",
       "      <td>31948</td>\n",
       "      <td>1</td>\n",
       "      <td>@user @user you don't have the balls to hashta...</td>\n",
       "      <td>you dont have the balls to hashtag me as a but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31948</th>\n",
       "      <td>31949</td>\n",
       "      <td>1</td>\n",
       "      <td>makes you ask yourself, who am i? then am i a...</td>\n",
       "      <td>makes you ask yourself who am i then am i anyb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>31961</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
       "      <td>vandalised in in  condemns act</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2242 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet  \\\n",
       "13        14      1  @user #cnn calls #michigan middle school 'buil...   \n",
       "14        15      1  no comment!  in #australia   #opkillingbay #se...   \n",
       "17        18      1                             retweet if you agree!    \n",
       "23        24      1    @user @user lumpy says i am a . prove it lumpy.   \n",
       "34        35      1  it's unbelievable that in the 21st century we'...   \n",
       "...      ...    ...                                                ...   \n",
       "31934  31935      1  lady banned from kentucky mall. @user  #jcpenn...   \n",
       "31946  31947      1  @user omfg i'm offended! i'm a  mailbox and i'...   \n",
       "31947  31948      1  @user @user you don't have the balls to hashta...   \n",
       "31948  31949      1   makes you ask yourself, who am i? then am i a...   \n",
       "31960  31961      1  @user #sikh #temple vandalised in in #calgary,...   \n",
       "\n",
       "                                             clean_tweet  \n",
       "13             calls middle school build the wall chant   \n",
       "14                                         no comment in  \n",
       "17                                  retweet if you agree  \n",
       "23                     lumpy says i am a  prove it lumpy  \n",
       "34     its unbelievable that in the st century wed ne...  \n",
       "...                                                  ...  \n",
       "31934                     lady banned from kentucky mall  \n",
       "31946         omfg im offended im a mailbox and im proud  \n",
       "31947  you dont have the balls to hashtag me as a but...  \n",
       "31948  makes you ask yourself who am i then am i anyb...  \n",
       "31960                     vandalised in in  condemns act  \n",
       "\n",
       "[2242 rows x 4 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daf0aac7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# clean the test data and append the cleaned tweets to the test data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_tweet \u001b[38;5;241m=\u001b[39m clean_tweets(\u001b[43mtest\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      3\u001b[0m test_tweet \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(test_tweet)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# append cleaned tweets to the training data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# test[\"clean_tweet\"] = test_tweet\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# compare the cleaned and uncleaned tweets\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "# clean the test data and append the cleaned tweets to the test data\n",
    "test_tweet = clean_tweets(test[\"tweet\"])\n",
    "test_tweet = pd.DataFrame(test_tweet)\n",
    "# append cleaned tweets to the training data\n",
    "# test[\"clean_tweet\"] = test_tweet\n",
    "\n",
    "# compare the cleaned and uncleaned tweets\n",
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c779faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# extract the labels from the train data\n",
    "y = train.label.values\n",
    "\n",
    "# use 70% for the training and 30% for the test\n",
    "x_train, x_test, y_train, y_test = train_test_split(train.clean_tweet.values, y, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=1, \n",
    "                                                    test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "59b6de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorize tweets for model building\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "\n",
    "# learn a vocabulary dictionary of all tokens in the raw documents\n",
    "vectorizer.fit(list(x_train) + list(x_test))\n",
    "# transform documents to document-term matrix\n",
    "x_train_vec = vectorizer.transform(x_train)\n",
    "x_test_vec = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "373caab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# classify using support vector classifier\n",
    "svm = svm.SVC(kernel = 'linear', probability=True)\n",
    "\n",
    "# fit the SVC model based on the given training data/ training the model using fit method \n",
    "svm.fit(x_train_vec, y_train)\n",
    "\n",
    "# perform classification and prediction on samples in x_test\n",
    "y_pred_svm = svm.predict(x_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55fffa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for SVC is:  94.86912086766085 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy score for SVC is: \", accuracy_score(y_test, y_pred_svm) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abcdb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f101ec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8773,  143],\n",
       "       [ 349,  324]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ade58bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c1470b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      8916\n",
      "           1       0.69      0.48      0.57       673\n",
      "\n",
      "    accuracy                           0.95      9589\n",
      "   macro avg       0.83      0.73      0.77      9589\n",
      "weighted avg       0.94      0.95      0.94      9589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "547080bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Hello MY name is Tushar '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "44ba6605",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [str(text1)]\n",
    "x_train_vec = vec_loaded.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "55519181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_vec = vectorizer.transform(x_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "122599dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'final_predict_model.sav'\n",
    "pickle.dump(svm, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c81616d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'vectorize_model.sav'\n",
    "pickle.dump(vectorizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad11719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'final_predict_model.sav'\n",
    "pickle.dump(svm, open(filename, 'wb'))\n",
    "a vec_loaded = pickle.load(open(filename,'rb'))   \n",
    "clf2 = pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d902859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a vec_loaded = pickle.load(open(filename,'rb'))   \n",
    "# clf2 = pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a63ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fe638bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 22482)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e7fb7d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 22482)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d317e218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.predict(x_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f31a733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "547a5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import tweepy\n",
    "import time\n",
    "import pandas as pd\n",
    "api_key =\"p9btWOq38TXczri4AfCSqJjkx\"\n",
    "# api secret key\n",
    "api_secret_key = \"6eVs0lKKfp6vfnNenY8wil8PmXlVSN7g1VNIVPawG8qXQf0dLf\"\n",
    "# access token\n",
    "access_token = \"1499855933974978570-apcsJqVHjqsxOhPl4wWTIhF3L9x9lc\"\n",
    "# access token secret\n",
    "access_token_secret = \"iILNydxaee0hl8WI8ZpOHL5MUqIWg3Or53nZnyZEMfGE2\"\n",
    "# authorize the API Key\n",
    "authentication = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "# authorization to user's access token and access token secret\n",
    "authentication.set_access_token(access_token, access_token_secret)\n",
    "# call the api\n",
    "api = tweepy.API(authentication, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ece1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_for_csv = []\n",
    "username = 'John'\n",
    "# for tweet in tweepy.Cursor(api.user_timeline, screen_name = username).items(5):\n",
    "#     print(str(tweet.full_text))\n",
    "# #     tweets_for_csv.append([username, tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a49a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @nelkboys: Were hot as a pistol\n",
      "What a crazy week. Thank you everyone for all the support. We got an army @KyleForgeard @stevewilldoit @BobMenery @Saliimthedream.\n",
      "🔥🔥 enjoy the @alienfrens event! 🖖🏼 https://t.co/u7oS2p5gs2\n",
      "Who are the Nelk Boys? Why Donald Trump appeared on a frat prank podcast https://t.co/lQFYMxruz6 via @YahooNews\n",
      "You’ll see these everywhere around Los Angeles… @fullsendpodcast. https://t.co/Eyezo2hvpR\n",
      "@iFungibility @happydad @KyleForgeard I can’t promise it’ll be the first but DM me and I’ll make sure we show her love. Got you and your family and tell her thank you from all of us. I’m on it.\n",
      "Good morning @TimJDillon 🤣🤣. https://t.co/RYBxdxvrp8\n",
      "Get ready for a new https://t.co/hTmmdxmhQk. Don’t forget what I know what to build.\n",
      "Crushed 😡\n",
      "Pretty sad if this what you learned  from a great 53 minute interview https://t.co/ZI249hlBDU. https://t.co/D0vu4EqQS9\n"
     ]
    }
   ],
   "source": [
    "for status in tweepy.Cursor(api.user_timeline,  tweet_mode='extended',screen_name=username).items(10):\n",
    "    print(status.full_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de137458",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'method'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m [status\u001b[38;5;241m.\u001b[39m_json \u001b[38;5;28;01mfor\u001b[39;00m status \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtweepy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCursor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m$EURUSD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtweet_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mextended\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Now you can iterate over 'results' and store the complete message from each tweet.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m my_tweets \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'method'"
     ]
    }
   ],
   "source": [
    "results = [status._json for status in tweepy.Cursor(API.search, q=\"$EURUSD\", count=1000, tweet_mode='extended', lang='en').items()]\n",
    "# Now you can iterate over 'results' and store the complete message from each tweet.\n",
    "my_tweets = []\n",
    "for result in results:\n",
    "    my_tweets.append(result[\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd598b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_related_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b462909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_tweets(text_query):\n",
    "    # list to store tweets\n",
    "    tweets_list = []\n",
    "    # no of tweets\n",
    "    count = 50\n",
    "    try:\n",
    "        # Pulling individual tweets from query\n",
    "        for tweet in api.search(q=text_query, count=count):\n",
    "            print(tweet.text)\n",
    "            # Adding to list that contains all tweets\n",
    "            tweets_list.append({'created_at': tweet.created_at,\n",
    "                                'tweet_id': tweet.id,\n",
    "                                'tweet_text': tweet.text})\n",
    "        return pd.DataFrame.from_dict(tweets_list)\n",
    "    \n",
    "    except BaseException as e:\n",
    "        print('failed on_status,', str(e))\n",
    "#         time.sleep(3)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9aa94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])|(\\%)|(\\$)|(\\>)|(\\<)|(\\{)|(\\})\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s/><br\\s/?)|(-)|(/)|(:).\")\n",
    "\n",
    "import preprocessor as p\n",
    "\n",
    "def clean_tweets(df):\n",
    "  tempArr = []\n",
    "  for line in df:\n",
    "    # send to tweet_processor\n",
    "    tmpL = p.clean(line)\n",
    "    # remove puctuation\n",
    "    tmpL = REPLACE_NO_SPACE.sub(\"\", tmpL.lower()) # convert all tweets to lower cases\n",
    "    tmpL = REPLACE_WITH_SPACE.sub(\" \", tmpL)\n",
    "    tempArr.append(tmpL)\n",
    "  return tempArr\n",
    "\n",
    "train_tweet = clean_tweets(train[\"tweet\"])\n",
    "train_tweet = pd.DataFrame(train_tweet)\n",
    "\n",
    "train[\"clean_tweet\"] = train_tweet\n",
    "\n",
    "# clean the test data and append the cleaned tweets to the test data\n",
    "test_tweet = clean_tweets(test[\"tweet\"])\n",
    "test_tweet = pd.DataFrame(test_tweet)\n",
    "# append cleaned tweets to the training data\n",
    "test[\"clean_tweet\"] = test_tweet\n",
    "\n",
    "# compare the cleaned and uncleaned tweets\n",
    "test.tail()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# extract the labels from the train data\n",
    "y = train.label.values\n",
    "\n",
    "# use 70% for the training and 30% for the test\n",
    "x_train, x_test, y_train, y_test = train_test_split(train.clean_tweet.values, y, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=1, \n",
    "                                                    test_size=0.3, shuffle=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vectorize tweets for model building\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "\n",
    "# learn a vocabulary dictionary of all tokens in the raw documents\n",
    "vectorizer.fit(list(x_train) + list(x_test))\n",
    "\n",
    "# transform documents to document-term matrix\n",
    "x_train_vec = vectorizer.transform(x_train)\n",
    "x_test_vec = vectorizer.transform(x_test)\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "# classify using support vector classifier\n",
    "svm = svm.SVC(kernel = 'linear', probability=True)\n",
    "# fit the SVC model based on the given training data/ training the model using fit method \n",
    "prob = svm.fit(x_train_vec, y_train)\n",
    "# perform classification and prediction on samples in x_test\n",
    "y_pred_svm = svm.predict(x_test_vec)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy score for SVC is: \", accuracy_score(y_test, y_pred_svm) * 100, '%')\n",
    "\n",
    "\n",
    "\n",
    "#Updated Code Here\n",
    "#Saving SVM model \n",
    "filename = 'final_predict_model.sav'\n",
    "print(\"Saving Modelss....\")\n",
    "pickle.dump(svm, open(filename, 'wb'))\n",
    "\n",
    "#Saving Vectorization Model \n",
    "\n",
    "filename = 'vectorize_model.sav'\n",
    "pickle.dump(vectorizer, open(filename, 'wb'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ae731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8886dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e514da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755139d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5067e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
